{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ipJ71NahQSaN",
        "outputId": "8d253f63-a13a-4d7b-8506-5b5edcd62b28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "loAtTwm5QSaU",
        "outputId": "cb80f673-d43b-433a-fa02-9973e03106ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.9.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (23.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2024.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2bcGNtwYQSaV",
        "outputId": "0e279d07-5b4c-44ae-dbbc-0e4634c6b245"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fasttext in /usr/local/lib/python3.10/dist-packages (0.9.2)\n",
            "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.10/dist-packages (from fasttext) (2.11.1)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from fasttext) (67.7.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fasttext) (1.23.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install fasttext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jXGnT4ljQSaW",
        "outputId": "3f01911c-4a2d-4e4c-c460-f23cccf165dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s8G33llpQSaX",
        "outputId": "cf49001d-7ae0-42c6-83da-5c187116cb8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: NVIDIA A100-SXM4-40GB\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():\n",
        "\n",
        "    # Tell PyTorch to use the GPU.\n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QCumdpXnQSaZ"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import pandas as pd\n",
        "import csv\n",
        "import re\n",
        "import math\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from transformers import AutoTokenizer, AutoModel, AdamW\n",
        "import torch\n",
        "\n",
        "import fasttext\n",
        "from huggingface_hub import hf_hub_download\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import pandas as pd\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vbu0uJoHQSaa"
      },
      "outputs": [],
      "source": [
        "def transform_column(df_column):\n",
        "  scaler = MinMaxScaler()\n",
        "  scores = np.array(df_column)\n",
        "  scores = scores.reshape(-1,1)\n",
        "  scaler.fit(scores)\n",
        "  scores = scaler.transform(scores)\n",
        "  scores = np.float32(scores)\n",
        "  return scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hCJ3j8kLQSab",
        "outputId": "74092236-ab77-4943-ff9e-25f91634b03f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
          ]
        }
      ],
      "source": [
        "model_path = hf_hub_download(repo_id=\"facebook/fasttext-language-identification\", filename=\"model.bin\")\n",
        "model = fasttext.load_model(model_path)\n",
        "\n",
        "def is_romanian_post(row):\n",
        "  if model.predict(change_n_to_space(row[\"title\"]))[0][0] == '__label__ron_Latn':\n",
        "      return True\n",
        "  return False\n",
        "\n",
        "def change_n_to_space(text):\n",
        "  return text.replace('\\n', ' ')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = pd.read_csv(\"train.csv\")\n",
        "df_validation = pd.read_csv(\"validation.csv\")\n",
        "df_test = pd.read_csv(\"test.csv\")"
      ],
      "metadata": {
        "id": "YAiEjrWNsf2u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "auQF9z_mEkZJ"
      },
      "outputs": [],
      "source": [
        "# Get the lists of contents and their labels.\n",
        "### TRAIN ###\n",
        "contents_train = df_train.full_text.values\n",
        "labels_train = df_train.label.values\n",
        "\n",
        "\n",
        "### VALIDATION ###\n",
        "contents_validation = df_validation.full_text.values\n",
        "labels_validation = df_validation.label.values\n",
        "\n",
        "### TEST ###\n",
        "contents_test = df_test.full_text.values\n",
        "labels_test = df_test.label.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b58dNYy7QSaj"
      },
      "outputs": [],
      "source": [
        "def clean_text (text):\n",
        "  text = text.lower()\n",
        "  text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text)\n",
        "  text = re.sub(r'\\s+', ' ', text)\n",
        "  return text\n",
        "\n",
        "contents_train = [clean_text(text) for text in contents_train]\n",
        "contents_validation = [clean_text(text) for text in contents_validation]\n",
        "contents_test = [clean_text(text) for text in contents_test]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1rxCfoGLQSak",
        "outputId": "722dbb08-8a68-48a1-bc0c-08188bda741f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.23.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.2.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.4.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN\n",
            " Original:  dosarul gazelor ieftine adriean videanu a fost arestat preventiv \n",
            "['dosarul', 'gazelor', 'ieftine', 'adriean', 'videanu', 'a', 'fost', 'arestat', 'preventiv']\n",
            "VALIDATION\n",
            " Original:  se mai gasesc cinemauri clasice in propia cladire nu incluse intr un mall \n",
            "['se', 'mai', 'gasesc', 'cinemauri', 'clasice', 'in', 'propia', 'cladire', 'nu', 'incluse', 'intr', 'un', 'mall']\n",
            "TEST\n",
            " Original:  salut reddit a a cum spune i username ul i m looking for guidance sunt o persoan nev z toare care se chinuie s i g seasc job de mai bine de doi ani i nimic salut n primul r nd mi cer scuze dar poate ave i voi idei cum a putea s mi g sesc un job c eu nu tiu ce s mai fac i unde s aplic i mi cam pierd speran a n mod normal nu mi place s spun c sunt nev z toare automat oamenii spun f masaj apreciez persoanele care fac a a ceva dar nu e pentru mine a a cum it nu e pentru toat lumea problema e urm toarea aplic peste tot ejobs olx facebook linkedin etc lucrurile decurg bine p n c nd ntreb dac au v zut n cv c sunt nev z toare apoi lini te oh mi pare r u noi nu putem s te ajut m nu ne descurc m poate te sun m noi ncerc s le explic c sunt dispus s lucrez gratis timp de dou s pt m ni ca s vedem dac pot face ce trebuie sunt dispus s le testez sistemul ca s vedem dac e accesibil i dac nu ce trebuie f cut ca s fie accesibil dar nimic i nu nu aplic la chestii gen graphic designer engineer sau mai tiu eu ce aplic la call center mi cunosc abilit ile i tiu c e ceva ce pot s fac tiu rom n i englez pu in portughez folosesc un pc la nivel avansat tiu cum s caut online tot ce mi trece prin minte tiu i ceva html din p cate nu am experien pentru c nc nu am reu it s g sesc o firm dispus s mi ofere o ans for ele de munc nu are rost s vorbim despre a a ceva am ncercat s discut cu cei de acolo i o doamn care teoretic ar fi trebuit s m ajute s mi g sesc ceva m a l sat f r cuvinte r spunsul pe care l am primit a fost e mai bine dac stai pe banii pe care i prime ti de la stat lucru care nu e okay din punctul meu de vedere de ce s fac a a ceva dac sunt capabil s muncesc nu sunt perfect nu tiu cine e dar sunt dispus s nv are cineva idee ce pot s ncerc p n acum am aplicat la teleperformance emag vodafone orange godaddy telus sykes accenture webhelp dotlumen cgs yardi i multe altele mul umesc \n",
            "['salut', 'reddit', 'a', 'a', 'cum', 'spune', 'i', 'username', 'ul', 'i', 'm', 'looking', 'for', 'guidance', 'sunt', 'o', 'persoan', 'nev', 'z', 'toare', 'care', 'se', 'chinuie', 's', 'i', 'g', 'seasc', 'job', 'de', 'mai', 'bine', 'de', 'doi', 'ani', 'i', 'nimic', 'salut', 'n', 'primul', 'r', 'nd', 'mi', 'cer', 'scuze', 'dar', 'poate', 'ave', 'i', 'voi', 'idei', 'cum', 'a', 'putea', 's', 'mi', 'g', 'sesc', 'un', 'job', 'c', 'eu', 'nu', 'tiu', 'ce', 's', 'mai', 'fac', 'i', 'unde', 's', 'aplic', 'i', 'mi', 'cam', 'pierd', 'speran', 'a', 'n', 'mod', 'normal', 'nu', 'mi', 'place', 's', 'spun', 'c', 'sunt', 'nev', 'z', 'toare', 'automat', 'oamenii', 'spun', 'f', 'masaj', 'apreciez', 'persoanele', 'care', 'fac', 'a', 'a', 'ceva', 'dar', 'nu', 'e', 'pentru', 'mine', 'a', 'a', 'cum', 'it', 'nu', 'e', 'pentru', 'toat', 'lumea', 'problema', 'e', 'urm', 'toarea', 'aplic', 'peste', 'tot', 'ejobs', 'olx', 'facebook', 'linkedin', 'etc', 'lucrurile', 'decurg', 'bine', 'p', 'n', 'c', 'nd', 'ntreb', 'dac', 'au', 'v', 'zut', 'n', 'cv', 'c', 'sunt', 'nev', 'z', 'toare', 'apoi', 'lini', 'te', 'oh', 'mi', 'pare', 'r', 'u', 'noi', 'nu', 'putem', 's', 'te', 'ajut', 'm', 'nu', 'ne', 'descurc', 'm', 'poate', 'te', 'sun', 'm', 'noi', 'ncerc', 's', 'le', 'explic', 'c', 'sunt', 'dispus', 's', 'lucrez', 'gratis', 'timp', 'de', 'dou', 's', 'pt', 'm', 'ni', 'ca', 's', 'vedem', 'dac', 'pot', 'face', 'ce', 'trebuie', 'sunt', 'dispus', 's', 'le', 'testez', 'sistemul', 'ca', 's', 'vedem', 'dac', 'e', 'accesibil', 'i', 'dac', 'nu', 'ce', 'trebuie', 'f', 'cut', 'ca', 's', 'fie', 'accesibil', 'dar', 'nimic', 'i', 'nu', 'nu', 'aplic', 'la', 'chestii', 'gen', 'graphic', 'designer', 'engineer', 'sau', 'mai', 'tiu', 'eu', 'ce', 'aplic', 'la', 'call', 'center', 'mi', 'cunosc', 'abilit', 'ile', 'i', 'tiu', 'c', 'e', 'ceva', 'ce', 'pot', 's', 'fac', 'tiu', 'rom', 'n', 'i', 'englez', 'pu', 'in', 'portughez', 'folosesc', 'un', 'pc', 'la', 'nivel', 'avansat', 'tiu', 'cum', 's', 'caut', 'online', 'tot', 'ce', 'mi', 'trece', 'prin', 'minte', 'tiu', 'i', 'ceva', 'html', 'din', 'p', 'cate', 'nu', 'am', 'experien', 'pentru', 'c', 'nc', 'nu', 'am', 'reu', 'it', 's', 'g', 'sesc', 'o', 'firm', 'dispus', 's', 'mi', 'ofere', 'o', 'ans', 'for', 'ele', 'de', 'munc', 'nu', 'are', 'rost', 's', 'vorbim', 'despre', 'a', 'a', 'ceva', 'am', 'ncercat', 's', 'discut', 'cu', 'cei', 'de', 'acolo', 'i', 'o', 'doamn', 'care', 'teoretic', 'ar', 'fi', 'trebuit', 's', 'm', 'ajute', 's', 'mi', 'g', 'sesc', 'ceva', 'm', 'a', 'l', 'sat', 'f', 'r', 'cuvinte', 'r', 'spunsul', 'pe', 'care', 'l', 'am', 'primit', 'a', 'fost', 'e', 'mai', 'bine', 'dac', 'stai', 'pe', 'banii', 'pe', 'care', 'i', 'prime', 'ti', 'de', 'la', 'stat', 'lucru', 'care', 'nu', 'e', 'okay', 'din', 'punctul', 'meu', 'de', 'vedere', 'de', 'ce', 's', 'fac', 'a', 'a', 'ceva', 'dac', 'sunt', 'capabil', 's', 'muncesc', 'nu', 'sunt', 'perfect', 'nu', 'tiu', 'cine', 'e', 'dar', 'sunt', 'dispus', 's', 'nv', 'are', 'cineva', 'idee', 'ce', 'pot', 's', 'ncerc', 'p', 'n', 'acum', 'am', 'aplicat', 'la', 'teleperformance', 'emag', 'vodafone', 'orange', 'godaddy', 'telus', 'sykes', 'accenture', 'webhelp', 'dotlumen', 'cgs', 'yardi', 'i', 'multe', 'altele', 'mul', 'umesc']\n"
          ]
        }
      ],
      "source": [
        "# fasttext embeddings for the dataset\n",
        "! pip install pandas numpy scikit-learn gensim nltk\n",
        "\n",
        "# Tokenization using nltk\n",
        "\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "from gensim.models import FastText\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "print(\"TRAIN\")\n",
        "# Print the original sentence.\n",
        "print(' Original: ', contents_train[0])\n",
        "contents_train_tokenized = [word_tokenize(text) for text in contents_train]\n",
        "print (contents_train_tokenized[0])\n",
        "\n",
        "print(\"VALIDATION\")\n",
        "# Print the original sentence.\n",
        "print(' Original: ', contents_validation[0])\n",
        "contents_validation_tokenized = [word_tokenize(text) for text in contents_validation]\n",
        "print (contents_validation_tokenized[0])\n",
        "\n",
        "print(\"TEST\")\n",
        "# Print the original sentence.\n",
        "print(' Original: ', contents_test[0])\n",
        "contents_test_tokenized = [word_tokenize(text) for text in contents_test]\n",
        "print (contents_test_tokenized[0])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QegbsMkfQSal"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "\n",
        "def compute_metrics(y_pred, y_true):\n",
        "\n",
        "  y_pred = y_pred.flatten()\n",
        "  macro_f1 = f1_score(y_true, y_pred, average='macro')\n",
        "  # Popular - label 1\n",
        "  TP_pop = 0 # y_true = 1 si y_pred = 1\n",
        "  FP_pop = 0 # y_true = 0 si y_pred = 1\n",
        "  FN_pop = 0 # y_true = 1 si y_pred = 0\n",
        "  # Unpopular - label 0\n",
        "  TP_unpop = 0 # y_true = 0 si y_pred = 0\n",
        "  FP_unpop = 0 # y_true = 1 si y_pred = 0\n",
        "  FN_unpop = 0 # y_true = 0 si y_pred = 1\n",
        "  n = len(y_pred)\n",
        "  for index in range(n):\n",
        "    # Popular\n",
        "    if y_true[index] == 1:\n",
        "      if y_pred[index] == 1:\n",
        "        TP_pop += 1\n",
        "      if y_pred[index] == 0:\n",
        "        FN_pop += 1\n",
        "        FP_unpop += 1\n",
        "    else:\n",
        "    # Unpopular y_true[index] == 0\n",
        "      if y_pred[index] == 0:\n",
        "        TP_unpop += 1\n",
        "      if y_pred[index] == 1:\n",
        "        FN_unpop += 1\n",
        "        FP_pop += 1\n",
        "\n",
        "  P_pop = 0 if TP_pop + FP_pop == 0 else TP_pop / (TP_pop + FP_pop)\n",
        "  R_pop = 0 if TP_pop + FN_pop == 0 else TP_pop / (TP_pop + FN_pop)\n",
        "  P_unpop = 0 if TP_unpop + FP_unpop == 0 else TP_unpop / (TP_unpop + FP_unpop)\n",
        "  R_unpop = 0 if TP_unpop + FN_unpop == 0 else TP_unpop / (TP_unpop + FN_unpop)\n",
        "\n",
        "  return (macro_f1, P_pop, R_pop, P_unpop, R_unpop)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# random forrest classification\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Use TF-IDF to convert text data to numerical features\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(min_df=5,\n",
        "                             max_df=0.8,\n",
        "                             sublinear_tf=True,\n",
        "                             use_idf=True)\n",
        "\n",
        "contents_train_tokenized = [' '.join(text) for text in contents_train_tokenized]\n",
        "\n",
        "contents_validation_tokenized = [' '.join(text) for text in contents_validation_tokenized]\n",
        "\n",
        "contents_test_tokenized = [' '.join(text) for text in contents_test_tokenized]\n",
        "\n",
        "# Apply the vectorizer\n",
        "contents_train_tokenized = vectorizer.fit_transform(contents_train_tokenized)\n",
        "contents_validation_tokenized = vectorizer.transform(contents_validation_tokenized)\n",
        "contents_test_tokenized = vectorizer.transform(contents_test_tokenized)"
      ],
      "metadata": {
        "id": "ZMHCFh76Ahp5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the model on training data\n",
        "rf.fit(contents_train_tokenized, labels_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "lZarleALL-Z7",
        "outputId": "b1f1b8e5-cefb-48e8-82b6-09ea36a13bc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(random_state=42)"
            ],
            "text/html": [
              "<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=42)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 156
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the forest's predict method on the validation data\n",
        "\n",
        "predictions_validation = rf.predict(contents_validation_tokenized)\n",
        "\n",
        "# Calculate the absolute errors\n",
        "\n",
        "errors = abs(predictions_validation - labels_validation)\n",
        "\n",
        "print('Mean Absolute Error:', round(np.mean(errors), 2), 'degrees.')\n",
        "\n",
        "predictions_validation.flatten()\n",
        "# # Evaluate using compute_metrics function\n",
        "macro_f1, P_popular, R_popular, P_unpopular, R_unpopular = compute_metrics(predictions_validation, labels_validation)\n",
        "accuracy_validation = accuracy_score(labels_validation, predictions_validation)\n",
        "print(\"VALIDATION\")\n",
        "print(\"Accuracy: \", accuracy_validation)\n",
        "print(\"Macro F1: \", macro_f1)\n",
        "print(\"Popular class: \")\n",
        "print(\"Precision: \", P_popular)\n",
        "print(\"Recall: \", R_popular)\n",
        "print(\"Unpopular class: \")\n",
        "print(\"Precision: \", P_unpopular)\n",
        "print(\"Recall: \", R_unpopular)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QYH_ZsjtJJCQ",
        "outputId": "2f87884f-88d4-4820-e1d6-fa301afbcf2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Absolute Error: 0.35 degrees.\n",
            "VALIDATION\n",
            "Accuracy:  0.6535731187884525\n",
            "Macro F1:  0.6395340815869939\n",
            "Popular class: \n",
            "Precision:  0.7519500780031201\n",
            "Recall:  0.45687203791469194\n",
            "Unpopular class: \n",
            "Precision:  0.610733695652174\n",
            "Recall:  0.8497164461247637\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prediction on test set\n",
        "\n",
        "predictions_test = rf.predict(contents_test_tokenized)\n",
        "\n",
        "# Calculate the absolute errors\n",
        "\n",
        "errors = abs(predictions_test - labels_test)\n",
        "\n",
        "print('Mean Absolute Error:', round(np.mean(errors), 2), 'degrees.')\n",
        "\n",
        "# Evaluate using compute_metrics function\n",
        "macro_f1, P_popular, R_popular, P_unpopular, R_unpopular = compute_metrics(predictions_test, labels_test)\n",
        "accuracy_test = accuracy_score(labels_test, predictions_test)\n",
        "print(\"TEST\")\n",
        "print(\"Accuracy: \", accuracy_test)\n",
        "print(\"Macro F1: \", macro_f1)\n",
        "print(\"Popular class: \")\n",
        "print(\"Precision: \", P_popular)\n",
        "print(\"Recall: \", R_popular)\n",
        "print(\"Unpopular class: \")\n",
        "print(\"Precision: \", P_unpopular)\n",
        "print(\"Recall: \", R_unpopular)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dqFB-xp0nt6z",
        "outputId": "9deaf38c-1402-46e9-d900-5128b83ecb33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Absolute Error: 0.42 degrees.\n",
            "TEST\n",
            "Accuracy:  0.5759897828863346\n",
            "Macro F1:  0.5729864179396502\n",
            "Popular class: \n",
            "Precision:  0.5897959183673469\n",
            "Recall:  0.4931740614334471\n",
            "Unpopular class: \n",
            "Precision:  0.5661066471877283\n",
            "Recall:  0.6584536958368734\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}